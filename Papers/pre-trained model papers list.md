## 预训练模型相关资料列表

#### 基础

#### 基础类

1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(https://aclanthology.org/N19-1423/)
2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(https://nlp.stanford.edu/seminar/details/jdevlin.pdf)


### Pre-trained Model-related Papers

#### 综述类：

1. Revisiting Pre-Trained Models for Chinese Natural Language Processing，**EMNLP 2020** (https://www.aclweb.org/anthology/2020.findings-emnlp.58.pdf)

2. Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks，**ACL2020**（https://arxiv.org/pdf/2004.10964.pdf）

   

#### 模型优化类

1. RoBERTa: A Robustly Optimized BERT Pretraining Approach，**EMNLP 2020**（https://arxiv.org/pdf/1907.11692.pdf）鲁棒性优化
2. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks，**EMNLP 2019** (https://www.aclweb.org/anthology/D19-1410.pdf)准确性和效率优化
3. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension，**ACL 2020 **(https://www.aclweb.org/anthology/2020.acl-main.703.pdf)降噪
4. Parameter-Efficient Transfer Learning for NLP，**ICML2019**  （https://arxiv.org/pdf/1902.00751.pdf）参数训练优化
5. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators(https://arxiv.org/pdf/2003.10555.pdf)

   

#### 应用类：

1. BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision，**KDD 2020**（https://arxiv.org/pdf/2006.15509.pdf）NER

2. Pre-training on Large-Scale Heterogeneous Graph, **KDD 2021**(http://www.shichuan.org/doc/111.pdf)

   

#### 领域类：

1. Spelling Error Correction with Soft-Masked BERT，**ACL 2020**（https://arxiv.org/pdf/2005.07421.pdf）文本纠错
2. E-BERT: Adapting BERT to E-commerce with Adaptive Hybrid Masking and Neighbor Product Reconstruction （https://arxiv.org/pdf/2009.02835.pdf) 电商
3. BERTweet: A pre-trained language model for English Tweets，**EMNLP 2020**（https://arxiv.org/pdf/2005.10200.pdf）Tweet





### 其他学习链接：

#### 教程类

1. NLPCC 《预训练语言模型回顾》讲习班报告（http://ymcui.com/talk/20201014_NLPCC2020_PLM_slides.pdf）**（重要）**

2. BERT Based Named Entity Recognition (NER) Tutorial And Demo（https://www.pragnakalp.com/bert-named-entity-recognition-ner-tutorial-demo/）

3. BERT for Keyphrase Extraction (Pytorch)（https://github.com/thunlp/BERT-KPE）

4. 一文了解预训练语言模型（https://mp.weixin.qq.com/s/cFk1h1HCE5TqiLZ7UYF15A）**推荐**

5. 预训练模型最新综述：过去、现在和未来(https://mp.weixin.qq.com/s/F4xUMY5nSb1ObpHtyAkzWg)

6. KDD2021| 大规模异质图上的预训练框架(https://mp.weixin.qq.com/s/SdUxo00cpb-u0arkaQtqVA)

7. HuggingFace BERT源码详解：基本模型组件实现(https://mp.weixin.qq.com/s/I4KSerhkFJdnIIpaZuf08A)

8. 关于 NLP 综述的综述(https://mp.weixin.qq.com/s/3DvXDLFQJnIrXFWVGowTjw)

9. Huggingface BERT源码详解：应用模型与训练优化(https://mp.weixin.qq.com/s/gR2i8HvyRBUxofovQarLNw)

   


#### 应用类

1. 预训练模型与知识图谱相结合的研究进展（https://mp.weixin.qq.com/s/3hAm2_Lxgf0o7BjRd7UxIA）

2. 关系抽取预训练综述（https://mp.weixin.qq.com/s/ZDsBuCqKKIvi2ZbPMhNEOA）

3. 预训练在小米的推理优化落地（https://mp.weixin.qq.com/s/itOyETgKBoRHOrIfKuphrw）

   

#### 工具类

1. 谷歌开源Embedding可视化工具（https://mp.weixin.qq.com/s/0eThEnoc-WNmsM6E5avTnA）

   

#### 拓展资料

1. BERT-related Papers（https://www.ctolib.com/amp/tomohideshibata-BERT-related-papers.html)
2. Compressing BERT for faster prediction（https://blog.rasa.com/compressing-bert-for-faster-prediction-2/）
3. Bert: 从架构优化、模型压缩到模型蒸馏最新进展详解(https://zhuanlan.zhihu.com/p/89022055)
4. 预训练语言模型：还能走多远？（https://mp.weixin.qq.com/s/suJALdfSy7KZN5vm265y-g）
5. 预训练语言模型研究进展和趋势展望（https://mp.weixin.qq.com/s/yQV8WK2ZfWt4FGEZ1ICCCQ）
6. KDD2021| 大规模异质图上的预训练框架(https://mp.weixin.qq.com/s/SdUxo00cpb-u0arkaQtqVA)
7. HuggingFace BERT源码详解：基本模型组件实现(https://mp.weixin.qq.com/s/I4KSerhkFJdnIIpaZuf08A)
8. Transformer统治的时代，LSTM模型并没有被代替，LSTM比Tranformer优势在哪里？(https://www.zhihu.com/question/439243827)

